{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c503_WIro-jB",
        "outputId": "602088ee-6366-461e-9146-aaf5a0f235ad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 7.9 MB 5.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 2.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 370 kB 5.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 482 kB 30.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 41 kB 405 kB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -q torch-scatter -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "!pip install -q torch-sparse -f https://data.pyg.org/whl/torch-1.10.0+cu113.html\n",
        "!pip install -q torch-geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "66Fv_AAtb2nC"
      },
      "outputs": [],
      "source": [
        "!wget -q https://github.com/abojchevski/sparse_smoothing/raw/master/sparse_smoothing/sparsegraph.py\n",
        "!mkdir data && cd data && wget -q https://github.com/abojchevski/sparse_smoothing/raw/master/data/cora_ml.npz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "cWoyQLW80PFX"
      },
      "outputs": [],
      "source": [
        "from statsmodels.stats.proportion import proportion_confint\n",
        "from sparsegraph import SparseGraph\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FLc3rJK04Byr"
      },
      "outputs": [],
      "source": [
        "seed = 5\n",
        "np.random.seed(seed) # Set the random seed of numpy for the data split.\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "DH9MY7oJ2YUw"
      },
      "outputs": [],
      "source": [
        "# Hyper-parameters\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "p_plus  = 0.001\n",
        "p_minus = 0.06\n",
        "alpha = 0.01\n",
        "r_a = 3\n",
        "r_d = 4\n",
        "n_samples = 100\n",
        "n_samples_eval = 50\n",
        "batch_size = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ukFvhVp6mY_T"
      },
      "outputs": [],
      "source": [
        "def load_and_standardize(file_name):\n",
        "    \"\"\"\n",
        "    Run gust.standardize() + make the attributes binary.\n",
        "    Parameters\n",
        "    ----------\n",
        "    file_name\n",
        "        Name of the file to load.\n",
        "    Returns\n",
        "    -------\n",
        "    graph: gust.SparseGraph\n",
        "        The standardized graph\n",
        "    \"\"\"\n",
        "    with np.load(file_name, allow_pickle=True) as loader:\n",
        "        loader = dict(loader)\n",
        "        if 'type' in loader:\n",
        "            del loader['type']\n",
        "        graph = SparseGraph.from_flat_dict(loader)\n",
        "    \n",
        "    graph.standardize()\n",
        "    \n",
        "    # binarize\n",
        "    graph._flag_writeable(True)\n",
        "    graph.adj_matrix[graph.adj_matrix != 0] = 1\n",
        "    graph.attr_matrix[graph.attr_matrix != 0] = 1\n",
        "    graph._flag_writeable(False)\n",
        "    return graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EivPGd44syV2",
        "outputId": "1a988aea-dc46-4e75-c26d-bcbebf814e9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number nodes:  2810 feature dim:  2879 num_classes:  7\n"
          ]
        }
      ],
      "source": [
        "# Load dataset\n",
        "graph = load_and_standardize('data/cora_ml.npz')\n",
        "n, d = graph.attr_matrix.shape\n",
        "nc = graph.labels.max() + 1\n",
        "print('number nodes: ', n, 'feature dim: ', d, 'num_classes: ', nc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hYmX0GC7Y-Bn"
      },
      "outputs": [],
      "source": [
        "def perturb_phi(X, p_plus, p_minus, batchsize=1):\n",
        "    n, d = X.shape\n",
        "    X = X[None,:,:].repeat(batchsize, axis=0)\n",
        "    binomial_minus = np.random.binomial(1, p_minus, size=(batchsize,n,d))\n",
        "    binomial_plus  = np.random.binomial(1, p_plus,  size=(batchsize,n,d))\n",
        "    epsilon = np.where(X==0, binomial_minus, binomial_plus)\n",
        "    output = np.logical_xor(X, epsilon).astype('int')\n",
        "    return output\n",
        "\n",
        "# outs = perturb_phi(graph.attr_matrix.toarray(), p_plus, p_minus, batchsize=2)\n",
        "# outs.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Sm5H4KiXsyY7"
      },
      "outputs": [],
      "source": [
        "msk = np.random.rand(n) < 0.8\n",
        "train_mask = msk\n",
        "test_mask = ~msk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "X4fDEAqZ7-0O"
      },
      "outputs": [],
      "source": [
        "# Train the given model on the given graph for num_epochs\n",
        "def train(model, data, train_mask, test_mask, p_plus, p_minus, num_epochs, nsamples=1, batchsize=1, device=None):\n",
        "    \n",
        "    # Important: add self-edges so a node depends on itself\n",
        "    adj = torch.tensor(data.adj_matrix.toarray())\n",
        "    adj += torch.eye(adj.shape[0])\n",
        "    adj = adj.to(device)\n",
        "    data_labels = torch.tensor(data.labels[train_mask]).to(device)\n",
        "\n",
        "    # Set up the loss and the optimizer\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "    # A utility function to compute the accuracy\n",
        "    def get_acc(outs, y, mask):\n",
        "        return (outs[mask].argmax(dim=1) == y[mask]).sum().float() / mask.sum()\n",
        "\n",
        "    best_acc_val = -1\n",
        "    X = data.attr_matrix.toarray()\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        # Zero grads -> forward pass -> compute loss -> backprop\n",
        "        optimizer.zero_grad()\n",
        "        outs = torch.zeros([n, nc], dtype=torch.float, device=device)\n",
        "        if p_plus + p_minus > 0:\n",
        "            for i in range(0, nsamples, batchsize):\n",
        "                X_batch = perturb_phi(X, p_plus, p_minus, min(batchsize, nsamples-i))\n",
        "                X_batch = torch.FloatTensor(X_batch).to(device)\n",
        "                outs += model(X_batch, adj).sum(0)\n",
        "            outs /= nsamples\n",
        "        else:\n",
        "            outs = model(torch.FloatTensor(X).to(device), adj)\n",
        "        \n",
        "        loss = loss_fn(outs[train_mask], data_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Compute accuracies, print only if this is the best result so far\n",
        "        acc_test = get_acc(outs.cpu(), torch.tensor(data.labels), test_mask)\n",
        "        acc_train = get_acc(outs.cpu(), torch.tensor(data.labels), train_mask)\n",
        "        if acc_test > best_acc_val:\n",
        "            best_acc_val = acc_test\n",
        "            print(f'[Epoch {epoch+1}/{num_epochs}] Loss: {loss} | train: {acc_train:.3f} | Test: {acc_test:.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nLLoPSA68Hvi"
      },
      "outputs": [],
      "source": [
        "class GNNModel(nn.Module):\n",
        "    def __init__(self, num_layers=2, sz_in=2879, sz_hid=64, sz_out=7):\n",
        "        super().__init__()\n",
        "\n",
        "        \n",
        "        self.layers = nn.ModuleList([\n",
        "                                     nn.Linear(\n",
        "                                         sz_in if i == 0 else sz_hid, sz_out if i == num_layers-1 else sz_hid\n",
        "                                     ) for i in range(num_layers)\n",
        "        ])\n",
        "        \n",
        "\n",
        "    def forward(self, fts, adj):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            fts: [N, D] \n",
        "            adj: [N, N] \n",
        "\n",
        "        Returns:\n",
        "            new_fts: [N, 7]\n",
        "        \"\"\"\n",
        "        deg = adj.sum(axis=1, keepdim=True) # Degree of nodes, shape [N, 1]\n",
        "        \n",
        "        for i, layer in enumerate(self.layers):\n",
        "            fts = layer(fts)\n",
        "            fts = adj @ fts / deg \n",
        "            if i < len(self.layers) -1:\n",
        "                fts = torch.relu(fts)\n",
        "        \n",
        "        return fts\n",
        "model = GNNModel()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x23RGZl4f-qd",
        "outputId": "d736fed3-07e6-4771-b501-fa456c3449eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 1/100] Loss: 1.9336978197097778 | train: 0.267 | Test: 0.280\n",
            "[Epoch 2/100] Loss: 3.5132558345794678 | train: 0.276 | Test: 0.288\n",
            "[Epoch 3/100] Loss: 1.872841238975525 | train: 0.383 | Test: 0.422\n",
            "[Epoch 15/100] Loss: 1.6278094053268433 | train: 0.396 | Test: 0.434\n",
            "[Epoch 16/100] Loss: 1.5941722393035889 | train: 0.439 | Test: 0.456\n",
            "[Epoch 17/100] Loss: 1.5565263032913208 | train: 0.507 | Test: 0.519\n",
            "[Epoch 18/100] Loss: 1.5172582864761353 | train: 0.561 | Test: 0.568\n",
            "[Epoch 19/100] Loss: 1.479609489440918 | train: 0.627 | Test: 0.617\n",
            "[Epoch 20/100] Loss: 1.4394080638885498 | train: 0.669 | Test: 0.645\n",
            "[Epoch 21/100] Loss: 1.3955748081207275 | train: 0.696 | Test: 0.675\n",
            "[Epoch 22/100] Loss: 1.3488879203796387 | train: 0.700 | Test: 0.684\n",
            "[Epoch 23/100] Loss: 1.2995319366455078 | train: 0.697 | Test: 0.688\n",
            "[Epoch 29/100] Loss: 1.0256916284561157 | train: 0.709 | Test: 0.690\n",
            "[Epoch 30/100] Loss: 0.9846044182777405 | train: 0.727 | Test: 0.700\n",
            "[Epoch 31/100] Loss: 0.9453845024108887 | train: 0.736 | Test: 0.716\n",
            "[Epoch 34/100] Loss: 0.8334681987762451 | train: 0.742 | Test: 0.718\n",
            "[Epoch 37/100] Loss: 0.7446318864822388 | train: 0.749 | Test: 0.722\n",
            "[Epoch 38/100] Loss: 0.7201341390609741 | train: 0.753 | Test: 0.728\n",
            "[Epoch 39/100] Loss: 0.696509599685669 | train: 0.762 | Test: 0.746\n",
            "[Epoch 40/100] Loss: 0.6740615367889404 | train: 0.774 | Test: 0.750\n",
            "[Epoch 41/100] Loss: 0.6534419655799866 | train: 0.788 | Test: 0.765\n",
            "[Epoch 42/100] Loss: 0.6324427723884583 | train: 0.802 | Test: 0.789\n",
            "[Epoch 43/100] Loss: 0.6142779588699341 | train: 0.818 | Test: 0.801\n",
            "[Epoch 44/100] Loss: 0.596685528755188 | train: 0.831 | Test: 0.819\n",
            "[Epoch 45/100] Loss: 0.5813974142074585 | train: 0.837 | Test: 0.824\n",
            "[Epoch 47/100] Loss: 0.549842119216919 | train: 0.842 | Test: 0.832\n",
            "[Epoch 48/100] Loss: 0.5357242822647095 | train: 0.850 | Test: 0.842\n",
            "[Epoch 49/100] Loss: 0.5205587148666382 | train: 0.856 | Test: 0.844\n",
            "[Epoch 50/100] Loss: 0.5075958967208862 | train: 0.862 | Test: 0.850\n",
            "[Epoch 54/100] Loss: 0.4579537510871887 | train: 0.878 | Test: 0.852\n",
            "[Epoch 58/100] Loss: 0.4158938229084015 | train: 0.887 | Test: 0.864\n",
            "[Epoch 61/100] Loss: 0.3889463543891907 | train: 0.897 | Test: 0.868\n",
            "[Epoch 63/100] Loss: 0.37118473649024963 | train: 0.895 | Test: 0.870\n",
            "[Epoch 64/100] Loss: 0.3638225495815277 | train: 0.901 | Test: 0.872\n",
            "[Epoch 65/100] Loss: 0.35593608021736145 | train: 0.899 | Test: 0.878\n",
            "[Epoch 74/100] Loss: 0.30495670437812805 | train: 0.907 | Test: 0.880\n",
            "[Epoch 76/100] Loss: 0.2961139380931854 | train: 0.914 | Test: 0.884\n",
            "[Epoch 78/100] Loss: 0.2879895269870758 | train: 0.917 | Test: 0.886\n"
          ]
        }
      ],
      "source": [
        "train(model, graph, train_mask, test_mask, p_plus, p_minus, num_epochs=100, nsamples=n_samples, batchsize=batch_size, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "PsKcfvbl8Hye"
      },
      "outputs": [],
      "source": [
        "def predict(model, data, train_mask, test_mask, p_plus, p_minus, nsamples=1, batchsize=1, device=None):\n",
        "    \n",
        "    # Important: add self-edges so a node depends on itself\n",
        "    adj = torch.tensor(data.adj_matrix.toarray())\n",
        "    adj += torch.eye(adj.shape[0])\n",
        "    adj = adj.to(device)\n",
        "\n",
        "    model.eval()\n",
        "    votes = torch.zeros((n, nc), dtype=torch.long, device=device)\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        X = data.attr_matrix.toarray()\n",
        "        if p_plus + p_minus > 0:\n",
        "            for i in tqdm(range(0, nsamples, batchsize), leave=False):\n",
        "                X_batch = perturb_phi(X, p_plus, p_minus, min(batchsize, nsamples-i))\n",
        "                X_batch = torch.FloatTensor(X_batch).to(device)\n",
        "                outs = model(X_batch, adj)\n",
        "                votes += F.one_hot(outs.argmax(2), nc).sum(0)\n",
        "        else:\n",
        "            outs = model(torch.FloatTensor(X).to(device), adj)\n",
        "            votes += F.one_hot(outs.argmax(1), nc)\n",
        "    return votes.cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mi3FYSUbw07M",
        "outputId": "5e9f9506-9371-48b7-d1fe-e91e001d139f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "data": {
            "text/plain": [
              "array([35677, 37422, 43883, 39370, 79339, 15007, 30302])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "votes = predict(model.to(device), graph, train_mask, test_mask, p_plus, p_minus, nsamples=n_samples, batchsize=batch_size, device=device)\n",
        "pre_votes = predict(model.to(device), graph, train_mask, test_mask, p_plus, p_minus, nsamples=n_samples_eval, batchsize=batch_size, device=device)\n",
        "votes.sum(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "UdgBwc1PCfps"
      },
      "outputs": [],
      "source": [
        "def p_lower_from_votes(votes, pre_votes, alpha, n_samples):\n",
        "    \"\"\"\n",
        "    Estimate a lower bound on the probability of the majority class using a Binomial confidence interval.\n",
        "    Parameters\n",
        "    ----------\n",
        "    votes: array_like [n_nodes, n_classes]\n",
        "        Votes per class for each sample\n",
        "    pre_votes: array_like [n_nodes, n_classes]\n",
        "        Votes (based on fewer samples) to determine the majority (and the second best) class\n",
        "    alpha : float\n",
        "        Significance level\n",
        "    n_samples : int\n",
        "        Number of MC samples\n",
        "    Returns\n",
        "    -------\n",
        "    p_lower: array-like [n_nodes]\n",
        "        Lower bound on the probability of the majority class\n",
        "    \"\"\"\n",
        "    # Multiple by 2 since we are only need a single side\n",
        "    n_best = votes[np.arange(votes.shape[0]), pre_votes.argmax(1)]\n",
        "    p_lower = proportion_confint(\n",
        "        n_best, n_samples, alpha=2 * alpha, method=\"beta\")[0]\n",
        "\n",
        "    return p_lower\n",
        "\n",
        "def compute_likelihood_ratio(q, r_a, r_d, p_plus, p_minus):\n",
        "    return ((p_plus/(1-p_minus)) ** (q-r_d)) * ((p_minus/(1-p_plus)) ** (q-r_a))\n",
        "\n",
        "def likelihood_ratio_and_R(p_plus, p_minus, r_a, r_d):\n",
        "    max_q = r_a + r_d\n",
        "    likelihood_ratio = np.zeros(max_q)\n",
        "    R = np.zeros(max_q)\n",
        "    R[0] = 1\n",
        "\n",
        "    for q in range(max_q):\n",
        "        likelihood_ratio[q] = compute_likelihood_ratio(q, r_a, r_d, p_plus, p_minus)\n",
        "        if q == 0:\n",
        "            continue\n",
        "        for i in range(1, q):\n",
        "            T_i = (r_a * ((p_plus/(1-p_plus)) ** i)) + (r_d * ((p_minus/(1-p_minus)) ** i))\n",
        "            R[q] += (-1**(-i+1)) * T_i * R[q-i]\n",
        "        R[q] /= q\n",
        "         \n",
        "    return R, likelihood_ratio\n",
        "\n",
        "def binary_certificate(p_plus, p_minus, r_a, r_d, p_lower):\n",
        "    \n",
        "    all_rho = np.zeros((n, r_a, r_d))\n",
        "\n",
        "    for n_ in range(len(p_lower)):\n",
        "        for ra in range(r_a):\n",
        "            for rd in range(r_d):\n",
        "                p = 0\n",
        "                rho = 0\n",
        "                if p_plus+p_minus < 1:\n",
        "                    start = 0\n",
        "                    end = r_a + r_d\n",
        "                else:\n",
        "                    start = r_a + r_d\n",
        "                    end = 0\n",
        "\n",
        "                R, likelihood_ratio = likelihood_ratio_and_R(p_plus, p_minus, r_a, r_d)\n",
        "                PB_phi_x = R * ((1-p_plus) ** ra) * ((1-p_minus) ** rd)\n",
        "                PB_phi_x_neigh = PB_phi_x/likelihood_ratio\n",
        "                    \n",
        "                for q in range(start, end):\n",
        "                    if p + PB_phi_x[q] > p_lower[n_]:\n",
        "                        break\n",
        "                    else:\n",
        "                        p += PB_phi_x[q]\n",
        "                        rho += PB_phi_x_neigh[q]\n",
        "                \n",
        "                    if p_lower[n_] - p > 0:\n",
        "                        rho += (p_lower[n_] - p)/likelihood_ratio[q]\n",
        "\n",
        "                all_rho[n_, ra, rd] = rho\n",
        "    return all_rho\n",
        "\n",
        "p_lower = p_lower_from_votes(votes, pre_votes, alpha, n_samples_eval)\n",
        "all_rho = binary_certificate(p_plus, p_minus, r_a, r_d, p_lower)\n",
        "# all_rho"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "l6wgjZLdqqBS",
        "outputId": "65766ff2-ac1f-4d44-ece6-f0fe8132d422"
      },
      "outputs": [],
      "source": [
        "heatmap = (all_rho>0.5).mean(0)\n",
        "sns.set_context('talk')\n",
        "sns.heatmap(heatmap, \n",
        "            cmap='Greys',\n",
        "            vmin=0, vmax=1, square=True, cbar_kws={\"shrink\": .5})\n",
        "plt.xlim(0, heatmap.shape[1])\n",
        "plt.ylim(0, heatmap.shape[0])\n",
        "plt.xlabel('$r_d$ radius')\n",
        "plt.ylabel('$r_a$ radius')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-L9INDx0Lc9"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Robustness_certificate_gnn_final.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
